# StarCoder Repository Analysis

This project analyzes the StarCoder dataset to identify repositories that appear in both V1 and V2 versions, then extracts merged pull requests that occurred between the two dataset snapshots.

## Overview

The project consists of two main scripts that work together:

1. **`dataset_loading.py`** - Downloads and processes StarCoder V1 and V2 datasets to extract repository information
2. **`github_repo_analysis.py`** - Analyzes overlapping repositories and finds merged pull requests between dataset snapshots

## What It Does

### Dataset Loading (`dataset_loading.py`)
- Connects to Hugging Face Hub using your authentication token
- Downloads the StarCoder V1 dataset (`bigcode/the-stack-dedup`) and V2 dataset (`bigcode/the-stack-v2-dedup`)
- Extracts repository names and their corresponding commit hashes
- Saves the data to CSV files:
  - `starcoder_v1_repos.csv` - Repository names and commit hashes from V1
  - `starcoder_v2_repos.csv` - Repository names and commit hashes from V2

### GitHub Repository Analysis (`github_repo_analysis.py`)
- Reads the CSV files generated by the dataset loading script
- Identifies repositories that appear in both V1 and V2 datasets
- **NEW: Language Filtering** - Only processes repositories that contain target programming languages
- **NEW: Keyword Filtering** - Only includes pull requests with specific keywords in titles or descriptions
- For each overlapping repository:
  - Applies language filter to skip repos without target languages
  - Fetches commit metadata from GitHub API to get commit dates
  - Retrieves all merged pull requests between the V1 and V2 commit dates
  - Applies keyword filters to include only relevant PRs
- Saves results to `filtered_merged_prs.csv` with columns:
  - `repo_name` - Repository name
  - `pr_number` - Pull request number
  - `pr_title` - Pull request title
  - `pr_url` - URL to the pull request
  - `merge_date` - When the PR was merged

## Prerequisites

### Required Python Packages
```bash
pip install huggingface_hub datasets python-dotenv requests
```

### Required Environment Variables
Create a `.env` file in the project root with:

```env
HF_TOKEN=your_huggingface_token_here
GITHUB_PAT=your_github_personal_access_token_here
```

#### Getting Your Tokens:

**Hugging Face Token:**
1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. Create a new token with "Read" access
3. Copy the token to your `.env` file

**GitHub Personal Access Token:**
1. Go to [GitHub Settings > Developer settings > Personal access tokens](https://github.com/settings/tokens)
2. Generate a new token (classic) with these scopes:
   - `public_repo` (to read public repository data)
   - `repo` (if you need access to private repos)
3. Copy the token to your `.env` file

## Usage

### Step 1: Load and Process Datasets
```bash
python dataset_loading.py
```

This script will:
- Authenticate with Hugging Face
- Download and process both StarCoder datasets
- Create `starcoder_v1_repos.csv` and `starcoder_v2_repos.csv`
- Display execution times for each dataset

**Note:** This step may take a significant amount of time as it processes large datasets with streaming enabled.

### Step 2: Configure Filters (Optional)
Edit the filtering configuration at the top of `github_repo_analysis.py`:

```python
TARGET_LANGUAGES = ["Python", "JavaScript", "TypeScript"]  # Only repos with these languages
TITLE_KEYWORDS = ["fix", "bug", "feature", "update", "refactor"]  # Only PRs with these words in title
BODY_KEYWORDS = ["performance", "optimization", "security", "test"]  # Only PRs with these words in body
```

### Step 3: Analyze Repositories and Extract PRs
```bash
python github_repo_analysis.py
```

This script will:
- Read the CSV files from Step 1
- Find overlapping repositories between V1 and V2
- Apply language filtering to skip repos without target languages
- Fetch commit dates from GitHub API
- Extract merged pull requests between the dataset snapshots
- Apply keyword filtering to include only relevant PRs
- Save results to `filtered_merged_prs.csv`

**Note:** This step includes rate limiting (1-second delays) to respect GitHub API limits.

## Output Files

### `starcoder_v1_repos.csv`
Contains repository names and commit hashes from StarCoder V1 dataset.

### `starcoder_v2_repos.csv`
Contains repository names and commit hashes from StarCoder V2 dataset.

### `filtered_merged_prs.csv`
Contains filtered merged pull requests that occurred between the V1 and V2 dataset snapshots for overlapping repositories. Only includes:
- Repositories that contain target programming languages
- Pull requests with keywords in titles or descriptions

## Rate Limiting and API Considerations

- **GitHub API**: The script includes 1-second delays between API calls to respect rate limits
- **Hugging Face**: Uses streaming to efficiently process large datasets
- **Error Handling**: Both scripts include error handling for API failures and missing data

## Performance Notes

- The dataset loading script processes data in streaming mode to handle large datasets efficiently
- The GitHub analysis script includes progress indicators and error handling
- Total execution time depends on the number of overlapping repositories and GitHub API response times

## Troubleshooting

### Common Issues:

1. **Authentication Errors**: Ensure your tokens are correctly set in the `.env` file
2. **Rate Limiting**: If you hit GitHub API limits, the script will skip problematic repositories
3. **Missing Repositories**: Some repositories may be deleted or made private between dataset snapshots
4. **Network Issues**: The scripts include retry logic, but persistent network issues may require manual intervention

### File Dependencies:
- `github_repo_analysis.py` requires the CSV files generated by `dataset_loading.py`
- Make sure to run `dataset_loading.py` first before running the analysis script

## Filtering Features

### Language Filtering
- Only processes repositories that contain any of the target programming languages
- Uses GitHub's `/languages` API endpoint to determine repository language composition
- Configurable via `TARGET_LANGUAGES` list in the script

### Keyword Filtering
- Filters pull requests based on keywords in titles and descriptions
- Supports separate keyword lists for titles and PR bodies
- Uses case-insensitive matching
- Configurable via `TITLE_KEYWORDS` and `BODY_KEYWORDS` lists in the script


