# StarCoder Repository Analysis

This project analyzes the StarCoder dataset to identify repositories that appear in both V1 and V2 versions, then extracts merged pull requests that occurred between the two dataset snapshots.

## Overview

The project consists of two main scripts that work together:

1. **`dataset_loading.py`** - Downloads and processes StarCoder V1 and V2 datasets to extract repository information
2. **`github_repo_analysis.py`** - Analyzes overlapping repositories and finds merged pull requests between dataset snapshots

## What It Does

### Dataset Loading (`dataset_loading.py`)
- Connects to Hugging Face Hub using your authentication token
- Downloads the StarCoder V1 dataset (`bigcode/the-stack-dedup`) and V2 dataset (`bigcode/the-stack-v2-dedup`)
- Extracts repository names and their corresponding commit hashes
- Saves the data to CSV files:
  - `starcoder_v1_repos.csv` - Repository names and commit hashes from V1
  - `starcoder_v2_repos.csv` - Repository names and commit hashes from V2

### GitHub Repository Analysis (`github_repo_analysis.py`)
- Reads the CSV files generated by the dataset loading script
- Identifies repositories that appear in both V1 and V2 datasets
- For each overlapping repository:
  - Fetches commit metadata from GitHub API to get commit dates
  - Retrieves all merged pull requests between the V1 and V2 commit dates
- Saves results to `merged_prs.csv` with columns:
  - `repo_name` - Repository name
  - `pr_number` - Pull request number
  - `pr_title` - Pull request title
  - `pr_url` - URL to the pull request
  - `merge_date` - When the PR was merged

## Prerequisites

### Required Python Packages
```bash
pip install huggingface_hub datasets python-dotenv requests
```

### Required Environment Variables
Create a `.env` file in the project root with:

```env
HF_TOKEN=your_huggingface_token_here
GITHUB_PAT=your_github_personal_access_token_here
```

#### Getting Your Tokens:

**Hugging Face Token:**
1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. Create a new token with "Read" access
3. Copy the token to your `.env` file

**GitHub Personal Access Token:**
1. Go to [GitHub Settings > Developer settings > Personal access tokens](https://github.com/settings/tokens)
2. Generate a new token (classic) with these scopes:
   - `public_repo` (to read public repository data)
   - `repo` (if you need access to private repos)
3. Copy the token to your `.env` file

## Usage

### Step 1: Load and Process Datasets
```bash
python dataset_loading.py
```

This script will:
- Authenticate with Hugging Face
- Download and process both StarCoder datasets
- Create `starcoder_v1_repos.csv` and `starcoder_v2_repos.csv`
- Display execution times for each dataset

**Note:** This step may take a significant amount of time as it processes large datasets with streaming enabled.

### Step 2: Analyze Repositories and Extract PRs
```bash
python github_repo_analysis.py
```

This script will:
- Read the CSV files from Step 1
- Find overlapping repositories between V1 and V2
- Fetch commit dates from GitHub API
- Extract merged pull requests between the dataset snapshots
- Save results to `merged_prs.csv`

**Note:** This step includes rate limiting (1-second delays) to respect GitHub API limits.

## Output Files

### `starcoder_v1_repos.csv`
Contains repository names and commit hashes from StarCoder V1 dataset.

### `starcoder_v2_repos.csv`
Contains repository names and commit hashes from StarCoder V2 dataset.

### `merged_prs.csv`
Contains all merged pull requests that occurred between the V1 and V2 dataset snapshots for overlapping repositories.

## Rate Limiting and API Considerations

- **GitHub API**: The script includes 1-second delays between API calls to respect rate limits
- **Hugging Face**: Uses streaming to efficiently process large datasets
- **Error Handling**: Both scripts include error handling for API failures and missing data

## Performance Notes

- The dataset loading script processes data in streaming mode to handle large datasets efficiently
- The GitHub analysis script includes progress indicators and error handling
- Total execution time depends on the number of overlapping repositories and GitHub API response times

## Troubleshooting

### Common Issues:

1. **Authentication Errors**: Ensure your tokens are correctly set in the `.env` file
2. **Rate Limiting**: If you hit GitHub API limits, the script will skip problematic repositories
3. **Missing Repositories**: Some repositories may be deleted or made private between dataset snapshots
4. **Network Issues**: The scripts include retry logic, but persistent network issues may require manual intervention

### File Dependencies:
- `github_repo_analysis.py` requires the CSV files generated by `dataset_loading.py`
- Make sure to run `dataset_loading.py` first before running the analysis script

## Data Privacy and Usage

- This tool only accesses public repository data
- No private code or sensitive information is stored
- All data processing is done locally
- Generated CSV files contain only public repository metadata and pull request information
